
  | Name       | Type             | Params | Mode
--------------------------------------------------------
0 | embedding  | Embedding        | 32     | train
1 | lstm1      | LSTM             | 576    | train
2 | activation | ReLU             | 0      | train
3 | lstm2      | LSTM             | 576    | train
4 | fc         | Linear           | 36     | train
5 | loss_fn    | CrossEntropyLoss | 0      | train
--------------------------------------------------------
1.2 K     Trainable params
0         Non-trainable params
1.2 K     Total params
0.005     Total estimated model params size (MB)
6         Modules in train mode
0         Modules in eval mode
Sanity Checking: |                                                                         | 0/? [00:00<?, ?it/s]
/home/cosmos/VScode Projects/coglab/torch_env/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.
Traceback (most recent call last):
  File "/home/cosmos/VScode Projects/coglab/NLP/pytorch-adaptive-computation-time/pytorch_adaptive_computation_time/associative-recurrent-memory-transformer/lstm_experimental/lstm_binary_reverse.py", line 207, in <module>
    train_model_reverse('/home/cosmos/VScode Projects/coglab/NLP/pytorch-adaptive-computation-time/pytorch_adaptive_computation_time/associative-recurrent-memory-transformer/lstm_experimental/data/binary_reverse.tsv')
  File "/home/cosmos/VScode Projects/coglab/NLP/pytorch-adaptive-computation-time/pytorch_adaptive_computation_time/associative-recurrent-memory-transformer/lstm_experimental/lstm_binary_reverse.py", line 200, in train_model_reverse
    trainer.fit(model, data_module)
  File "/home/cosmos/VScode Projects/coglab/torch_env/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/home/cosmos/VScode Projects/coglab/torch_env/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cosmos/VScode Projects/coglab/torch_env/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/cosmos/VScode Projects/coglab/torch_env/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/home/cosmos/VScode Projects/coglab/torch_env/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1023, in _run_stage
    self._run_sanity_check()
  File "/home/cosmos/VScode Projects/coglab/torch_env/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1052, in _run_sanity_check
    val_loop.run()
  File "/home/cosmos/VScode Projects/coglab/torch_env/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py", line 178, in _decorator
    return loop_run(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cosmos/VScode Projects/coglab/torch_env/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 128, in run
    batch, batch_idx, dataloader_idx = next(data_fetcher)
                                       ^^^^^^^^^^^^^^^^^^
  File "/home/cosmos/VScode Projects/coglab/torch_env/lib/python3.12/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
            ^^^^^^^^^^^^^^^^^^
  File "/home/cosmos/VScode Projects/coglab/torch_env/lib/python3.12/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
            ^^^^^^^^^^^^^^^^^^^
  File "/home/cosmos/VScode Projects/coglab/torch_env/lib/python3.12/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
          ^^^^^^^^^^^^^^^^^^^^
  File "/home/cosmos/VScode Projects/coglab/torch_env/lib/python3.12/site-packages/pytorch_lightning/utilities/combined_loader.py", line 142, in __next__
    out = next(self.iterators[0])
          ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cosmos/VScode Projects/coglab/torch_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
           ^^^^^^^^^^^^^^^^^
  File "/home/cosmos/VScode Projects/coglab/torch_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 673, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cosmos/VScode Projects/coglab/torch_env/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/cosmos/VScode Projects/coglab/NLP/pytorch-adaptive-computation-time/pytorch_adaptive_computation_time/associative-recurrent-memory-transformer/lstm_experimental/processing/reverse_collator.py", line 32, in __call__
    equal_token_tensor = torch.full((adjusted_inputs_tensor.size(0), 1), self.equal_token_id, dtype=torch.long)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: full() received an invalid combination of arguments - got (tuple, list, dtype=torch.dtype), but expected one of:
 * (tuple of ints size, Number fill_value, *, tuple of names names, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)
 * (tuple of ints size, Number fill_value, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)
