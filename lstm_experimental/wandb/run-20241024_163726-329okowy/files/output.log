/home/konstantin.smirnov/miniconda3/envs/project_env/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/konstantin.smirnov/associative-recurrent-memory-transformer/lstm_experimental/checkpoints/binary_copy exists and is not empty.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name       | Type             | Params | Mode
--------------------------------------------------------
0 | embedding  | Embedding        | 32     | train
1 | lstm1      | LSTM             | 70.7 K | train
2 | activation | ReLU             | 0      | train
3 | lstm2      | LSTM             | 132 K  | train
4 | lstm3      | LSTM             | 132 K  | train
5 | lstm4      | LSTM             | 132 K  | train
6 | fc         | Linear           | 516    | train
7 | loss_fn    | CrossEntropyLoss | 0      | train
--------------------------------------------------------
467 K     Trainable params
0         Non-trainable params
467 K     Total params
1.870     Total estimated model params size (MB)
8         Modules in train mode
0         Modules in eval mode
Epoch 11:  25%|████████████████████▉                                                              | 790/3125 [00:21<01:04, 36.04it/s, v_num=kowy]
/home/konstantin.smirnov/miniconda3/envs/project_env/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.
/home/konstantin.smirnov/miniconda3/envs/project_env/lib/python3.9/site-packages/pytorch_lightning/utilities/data.py:78: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 256. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
                                                                                                                                                 
/home/konstantin.smirnov/miniconda3/envs/project_env/lib/python3.9/site-packages/pytorch_lightning/utilities/data.py:78: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 160. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
Epoch 0, global step 3125: 'val_loss' reached 0.21975 (best 0.21975), saving model to '/home/konstantin.smirnov/associative-recurrent-memory-transformer/lstm_experimental/checkpoints/binary_copy/lenSEQ_LENGTH=0-best-checkpoint-val_loss=0.22.ckpt' as top 1
Epoch 1, global step 6250: 'val_loss' reached 0.03965 (best 0.03965), saving model to '/home/konstantin.smirnov/associative-recurrent-memory-transformer/lstm_experimental/checkpoints/binary_copy/lenSEQ_LENGTH=0-best-checkpoint-val_loss=0.04.ckpt' as top 1
Epoch 2, global step 9375: 'val_loss' reached 0.02120 (best 0.02120), saving model to '/home/konstantin.smirnov/associative-recurrent-memory-transformer/lstm_experimental/checkpoints/binary_copy/lenSEQ_LENGTH=0-best-checkpoint-val_loss=0.02.ckpt' as top 1
Epoch 3, global step 12500: 'val_loss' reached 0.01435 (best 0.01435), saving model to '/home/konstantin.smirnov/associative-recurrent-memory-transformer/lstm_experimental/checkpoints/binary_copy/lenSEQ_LENGTH=0-best-checkpoint-val_loss=0.01.ckpt' as top 1
Epoch 4, global step 15625: 'val_loss' reached 0.00238 (best 0.00238), saving model to '/home/konstantin.smirnov/associative-recurrent-memory-transformer/lstm_experimental/checkpoints/binary_copy/lenSEQ_LENGTH=0-best-checkpoint-val_loss=0.00.ckpt' as top 1
Epoch 5, global step 18750: 'val_loss' reached 0.00105 (best 0.00105), saving model to '/home/konstantin.smirnov/associative-recurrent-memory-transformer/lstm_experimental/checkpoints/binary_copy/lenSEQ_LENGTH=0-best-checkpoint-val_loss=0.00.ckpt' as top 1
Epoch 6, global step 21875: 'val_loss' was not in top 1
Epoch 7, global step 25000: 'val_loss' reached 0.00075 (best 0.00075), saving model to '/home/konstantin.smirnov/associative-recurrent-memory-transformer/lstm_experimental/checkpoints/binary_copy/lenSEQ_LENGTH=0-best-checkpoint-val_loss=0.00.ckpt' as top 1
Epoch 8, global step 28125: 'val_loss' reached 0.00025 (best 0.00025), saving model to '/home/konstantin.smirnov/associative-recurrent-memory-transformer/lstm_experimental/checkpoints/binary_copy/lenSEQ_LENGTH=0-best-checkpoint-val_loss=0.00.ckpt' as top 1
Epoch 9, global step 31250: 'val_loss' reached 0.00022 (best 0.00022), saving model to '/home/konstantin.smirnov/associative-recurrent-memory-transformer/lstm_experimental/checkpoints/binary_copy/lenSEQ_LENGTH=0-best-checkpoint-val_loss=0.00.ckpt' as top 1
Epoch 10, global step 34375: 'val_loss' reached 0.00021 (best 0.00021), saving model to '/home/konstantin.smirnov/associative-recurrent-memory-transformer/lstm_experimental/checkpoints/binary_copy/lenSEQ_LENGTH=0-best-checkpoint-val_loss=0.00.ckpt' as top 1
