LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name       | Type             | Params | Mode
--------------------------------------------------------
0 | embedding  | Embedding        | 32     | train
1 | lstm1      | LSTM             | 70.7 K | train
2 | activation | ReLU             | 0      | train
3 | lstm2      | LSTM             | 132 K  | train
4 | lstm3      | LSTM             | 132 K  | train
5 | lstm4      | LSTM             | 132 K  | train
6 | fc         | Linear           | 516    | train
7 | loss_fn    | CrossEntropyLoss | 0      | train
--------------------------------------------------------
467 K     Trainable params
0         Non-trainable params
467 K     Total params
1.870     Total estimated model params size (MB)
8         Modules in train mode
0         Modules in eval mode
Epoch 19:  32%|███████████████████████████▎                                                        | 65/200 [00:00<00:01, 114.02it/s, v_num=0har]
/home/konstantin.smirnov/miniconda3/envs/project_env/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.
/home/konstantin.smirnov/miniconda3/envs/project_env/lib/python3.9/site-packages/pytorch_lightning/utilities/data.py:78: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 32. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
/home/konstantin.smirnov/miniconda3/envs/project_env/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.
                                                                                                                                                 
Epoch 0, global step 200: 'val_loss' reached 0.69422 (best 0.69422), saving model to '/home/konstantin.smirnov/associative-recurrent-memory-transformer/lstm_experimental/checkpoints/binary_copy/len40-best-checkpoint-val_loss=0.69.ckpt' as top 1
Epoch 1, global step 400: 'val_loss' reached 0.69326 (best 0.69326), saving model to '/home/konstantin.smirnov/associative-recurrent-memory-transformer/lstm_experimental/checkpoints/binary_copy/len40-best-checkpoint-val_loss=0.69.ckpt' as top 1
Epoch 2, global step 600: 'val_loss' reached 0.69304 (best 0.69304), saving model to '/home/konstantin.smirnov/associative-recurrent-memory-transformer/lstm_experimental/checkpoints/binary_copy/len40-best-checkpoint-val_loss=0.69.ckpt' as top 1
Epoch 3, global step 800: 'val_loss' reached 0.67923 (best 0.67923), saving model to '/home/konstantin.smirnov/associative-recurrent-memory-transformer/lstm_experimental/checkpoints/binary_copy/len40-best-checkpoint-val_loss=0.68.ckpt' as top 1
Epoch 4, global step 1000: 'val_loss' was not in top 1
Epoch 5, global step 1200: 'val_loss' reached 0.63956 (best 0.63956), saving model to '/home/konstantin.smirnov/associative-recurrent-memory-transformer/lstm_experimental/checkpoints/binary_copy/len40-best-checkpoint-val_loss=0.64.ckpt' as top 1
Epoch 6, global step 1400: 'val_loss' reached 0.61130 (best 0.61130), saving model to '/home/konstantin.smirnov/associative-recurrent-memory-transformer/lstm_experimental/checkpoints/binary_copy/len40-best-checkpoint-val_loss=0.61.ckpt' as top 1
Epoch 7, global step 1600: 'val_loss' reached 0.53259 (best 0.53259), saving model to '/home/konstantin.smirnov/associative-recurrent-memory-transformer/lstm_experimental/checkpoints/binary_copy/len40-best-checkpoint-val_loss=0.53.ckpt' as top 1
Epoch 8, global step 1800: 'val_loss' reached 0.46519 (best 0.46519), saving model to '/home/konstantin.smirnov/associative-recurrent-memory-transformer/lstm_experimental/checkpoints/binary_copy/len40-best-checkpoint-val_loss=0.47.ckpt' as top 1
Epoch 9, global step 2000: 'val_loss' reached 0.40347 (best 0.40347), saving model to '/home/konstantin.smirnov/associative-recurrent-memory-transformer/lstm_experimental/checkpoints/binary_copy/len40-best-checkpoint-val_loss=0.40.ckpt' as top 1
Epoch 10, global step 2200: 'val_loss' reached 0.40158 (best 0.40158), saving model to '/home/konstantin.smirnov/associative-recurrent-memory-transformer/lstm_experimental/checkpoints/binary_copy/len40-best-checkpoint-val_loss=0.40.ckpt' as top 1
Epoch 11, global step 2400: 'val_loss' reached 0.24958 (best 0.24958), saving model to '/home/konstantin.smirnov/associative-recurrent-memory-transformer/lstm_experimental/checkpoints/binary_copy/len40-best-checkpoint-val_loss=0.25.ckpt' as top 1
Epoch 12, global step 2600: 'val_loss' was not in top 1
Epoch 13, global step 2800: 'val_loss' reached 0.19484 (best 0.19484), saving model to '/home/konstantin.smirnov/associative-recurrent-memory-transformer/lstm_experimental/checkpoints/binary_copy/len40-best-checkpoint-val_loss=0.19.ckpt' as top 1
Epoch 14, global step 3000: 'val_loss' reached 0.15727 (best 0.15727), saving model to '/home/konstantin.smirnov/associative-recurrent-memory-transformer/lstm_experimental/checkpoints/binary_copy/len40-best-checkpoint-val_loss=0.16.ckpt' as top 1
Epoch 15, global step 3200: 'val_loss' was not in top 1
Epoch 16, global step 3400: 'val_loss' reached 0.12375 (best 0.12375), saving model to '/home/konstantin.smirnov/associative-recurrent-memory-transformer/lstm_experimental/checkpoints/binary_copy/len40-best-checkpoint-val_loss=0.12.ckpt' as top 1
Epoch 17, global step 3600: 'val_loss' was not in top 1
Epoch 18, global step 3800: 'val_loss' reached 0.08191 (best 0.08191), saving model to '/home/konstantin.smirnov/associative-recurrent-memory-transformer/lstm_experimental/checkpoints/binary_copy/len40-best-checkpoint-val_loss=0.08.ckpt' as top 1

Detected KeyboardInterrupt, attempting graceful shutdown ...
